# Задача 1: Обеспечить разработку 

Для решения задачи предлагается использовать GitLab (SaaS-версия) как единую платформу для DevOps-цикла.

GitLab объединяет в себе хостинг репозиториев, продвинутый CI/CD, управление секретами и реестр контейнеров. Оно идеально покрывает все требования.

### 1. Архитектура и взаимодействие компонентов

Решение строится на базе облачного сервиса GitLab.com, который взаимодействует с нашими серверами через GitLab Runner.

* VCS: Каждый микросервис размещается в отдельном Git-репозитории в рамках одной группы или подгрупп.
* CI/CD Engine: Описывается файлом .gitlab-ci.yml в корне каждого проекта.
* Исполнители (Runners): Для выполнения тяжелых сборок или соблюдения требований безопасности используются собственные GitLab Runners, установленные на собственных серверах (On-premise).

### 2. Соответствие техническим требованиям

| Требование | Реализация в GitLab |
|-----------|---------------------|
| **Облачная система** | Используется GitLab.com (SaaS), не требующий поддержки собственной инфраструктуры для управляющего узла. |
| **Git & Репозиторий на сервис** | Нативная поддержка Git; структура Group/Project позволяет изолировать сервисы. |
| **Запуск по событию** | Триггеры на `push`, `merge request` или создание `tag` настраиваются через секцию `rules`. |
| **Запуск по кнопке с параметрами** | Функция **Run Pipeline**, где можно вручную вводить `variables` перед стартом. |
| **Настройки и шаблоны** | Использование ключевых слов `include` (подключение внешних файлов) и `extends` (наследование конфигураций). |
| **Безопасное хранение секретов** | Раздел **CI/CD Variables** с опциями **Masked** (скрытие в логах) и **Protected** (доступ только для защищённых веток). |
| **Несколько конфигураций** | Один YAML-файл может содержать множество `jobs`, которые запускаются в зависимости от условий (ветка, тег, переменная). |
| **Custom Docker images** | Параметр `image: my-custom-registry.com/my-build-image:latest` для каждого шага. |
| **Собственные агенты** | GitLab Runner устанавливается на любой сервер (Linux/Windows/K8s) и регистрируется в облачном GitLab. |
| **Параллельный запуск** | Настройка `concurrent` в конфиге раннера и использование `stages` позволяют запускать сборки параллельно. |
| **Параллельные тесты** | Ключевое слово `parallel: N` позволяет разбить тесты на несколько параллельных потоков внутри одной задачи. |

### 3. Описание процессов

**Хранение кода и версионирование**

Каждый сервис имеет свой жизненный цикл. Использование GitLab Groups позволяет гибко управлять правами доступа: например, команда "Backend" имеет доступ к одной группе репозиториев, а "Frontend" — к другой.

**Процесс сборки (CI)**

При пуше кода GitLab проверяет файл `.gitlab-ci.yml.`

* Шаблонизация: Создается один проект "CI-Templates", где храните эталонные шаги сборки для Docker, прогона тестов и линтеров. Другие проекты просто подключают их через include:project.
* Гибкость: Кастомные шаги описываются в (script:) для специфических задач (например, генерация документации или специфическая компиляция).
* Изоляция: Сборка идет внутри Docker-контейнера на собственном сервере, что гарантирует чистоту окружения.

**Секреты и безопасность**

Пароли к БД или ключи доступа к облаку не хранятся в коде. Они добавляются в настройки проекта/группы. GitLab также поддерживает интеграцию с HashiCorp Vault, если требования к безопасности станут еще жестче.

## Обоснование выбора

1. **Единая экосистема:** Не нужно настраивать интеграцию между GitHub и Jenkins или Bitbucket и TeamCity. Все — от кода до мониторинга деплоя — находится в одном окне.

1. **Гибридная модель:** Облачный интерфейс (SaaS) избавляет от необходимости администрирования, а собственные Runner-ы дают полный контроль над ресурсами и безопасностью (код не покидает внутренний контур, если раннер внутри сети).

1. **YAML-as-code:** Вся логика сборки версионируется вместе с кодом. Это позволяет легко откатывать изменения в процессе CI/CD.

1. **Масштабируемость:** Параллелизация тестов «из коробки» (keyword parallel) — одна из лучших на рынке, что критично для ускорения Time-to-Market.

# Задача 2: Логи

## Vector / Fluent Bit → RabbitMQ → OpenSearch → OpenSearch Dashboards

Данный вариант обеспечивает централизованный сбор, надёжную доставку и удобный анализ логов в микросервисной архитектуре.

### Архитектура решения

**Компоненты:**
- **Vector / Fluent Bit** — агент сбора логов на каждом хосте
- **RabbitMQ (quorum queues)** — промежуточный слой гарантированной доставки
- **OpenSearch** — центральное хранилище и поисковый движок
- **OpenSearch Dashboards** — пользовательский интерфейс для работы с логами

#### 1. Сбор логов в центральное хранилище со всех хостов

- На каждом хосте или ноде Kubernetes разворачивается агент **Vector** или **Fluent Bit**.
- Агент читает логи контейнеров из stdout/stderr (через Docker / containerd) либо из journald/файлов.
- Все логи направляются в единый кластер **OpenSearch**, выступающий центральным хранилищем.

#### 2. Минимальные требования к приложениям, сбор логов из stdout

- Приложения пишут логи исключительно в **stdout**.
- Интеграция с системой логирования на уровне кода не требуется.
- Рекомендуемый формат логов — JSON (для удобства фильтрации и поиска), но это не является обязательным условием.

#### 3. Гарантированная доставка логов до центрального хранилища

Надёжность доставки обеспечивается на двух уровнях.

##### 3.1 Буферизация на агенте
- Vector / Fluent Bit настраивается с **локальным дисковым буфером**.
- При временной недоступности RabbitMQ логи сохраняются на диске хоста и отправляются после восстановления соединения.

##### 3.2 RabbitMQ как устойчивый слой доставки
- Используются **quorum queues**, реплицируемые между несколькими нодами RabbitMQ.
- Индексатор (Logstash или Vector consumer) читает сообщения из очереди и подтверждает (ACK) их только после успешной обработки и записи в OpenSearch.
- При деградации или остановке OpenSearch сообщения накапливаются в очереди RabbitMQ.

**Семантика доставки:**  
- *At-least-once* — лог не теряется, возможны дубликаты, что допустимо для логирования.

#### 4. Поиск и фильтрация по записям логов

- **OpenSearch** обеспечивает:
  - полнотекстовый поиск по полю `message`,
  - фильтрацию по структурированным полям (`service`, `env`, `host`, `level`, `trace_id`),
  - агрегации и аналитические запросы.
- На этапе индексирования выполняется нормализация логов и обогащение метаданными.

#### 5. Пользовательский интерфейс и доступ разработчиков

- **OpenSearch Dashboards** предоставляет:
  - веб-интерфейс для поиска и анализа логов,
  - разграничение доступа (RBAC),
  - read-only доступ для разработчиков,
  - изоляцию по окружениям (prod/stage) или командам через индексы и тенанты.

#### 6. Ссылки на сохранённые поиски

- В OpenSearch Dashboards можно:
  - сохранять поисковые запросы (Saved Searches),
  - делиться permalink-ссылками на сохранённый поиск или текущее представление.
- Это позволяет передавать коллегам воспроизводимые результаты поиска логов.

## Итог

- централизованный сбор логов,
- минимальные требования к приложениям,
- гарантированная доставка,
- мощный поиск и фильтрация,
- удобный UI для разработчиков,
- возможность делиться сохранёнными поисками.

Решение хорошо масштабируется, устойчиво к сбоям и подходит для большинства микросервисных архитектур среднего и крупного размера.

# Задача 3: Мониторинг

**Prometheus + Grafana**

### Обоснование выбора

* Pull-модель: Prometheus сам забирает данные. Это защищает мониторинг от «лавинообразной» нагрузки, если сервисы начнут массово слать метрики при сбое.

* Labels (Метки): Каждая метрика имеет набор тегов (например, service="order", env="prod"). Это позволяет фильтровать и группировать данные любой сложности в одном запросе.

* Экосистема: Для большинства стандартных сервисов (PostgreSQL, Nginx, Kafka) уже существуют готовые «экспортеры» и преднастроенные дашборды в сообществе Grafana.

* Низкая нагрузка: Компоненты сбора (Node Exporter, cAdvisor) написаны на Go и потребляют минимальное количество ресурсов хоста.

# Задача 4: Логи * (необязательная)

https://github.com/netology-code/micros-homeworks/commit/b609022b7c72b63c789e4f490719abea5fc2ea59

# Задача 5: Мониторинг * (необязательная)

https://github.com/netology-code/micros-homeworks/commit/4ea012b0bfe49069d66708085fdde441cd528dcc
